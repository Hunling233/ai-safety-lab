import json
import requests
import streamlit as st

# Read backend address from secrets.toml
BACKEND = st.secrets.get("BACKEND_URL", "http://127.0.0.1:8000")

def display_suite_evidence(suite_name, evidence):
    """
    [Level 3] æ˜¾ç¤ºå•ä¸ªæµ‹è¯•è¿è¡Œçš„è¯¦ç»†å†…å®¹
    æ ¹æ®ä¸åŒçš„æµ‹è¯•å¥—ä»¶æ˜¾ç¤ºç›¸åº”çš„è¾“å…¥è¾“å‡ºå’Œè¯„åˆ†ä¿¡æ¯
    """
    # ç»Ÿä¸€çš„è¾“å…¥è¾“å‡ºæ˜¾ç¤º
    if 'score_rationale_audit' in suite_name:
        input_text = evidence.get('input', evidence.get('prompt', 'N/A'))
        model_output = evidence.get('model_output', {})
        if isinstance(model_output, dict):
            output_text = model_output.get('output', str(model_output))
        else:
            output_text = str(model_output)
    elif 'compliance' in suite_name.lower() or 'ethics' in suite_name.lower():
        # Compliance æµ‹è¯•ä½¿ç”¨ç‰¹å®šçš„å­—æ®µå
        input_text = evidence.get('prompt', 'N/A')
        output_text = evidence.get('model_output', 'N/A')
    elif 'adversarial' in suite_name.lower() or 'prompt_injection' in suite_name.lower():
        # Adversarial æµ‹è¯•ä½¿ç”¨ attack å­—æ®µä½œä¸ºè¾“å…¥
        input_text = evidence.get('attack', 'N/A')
        output_text = evidence.get('output', 'N/A')
    elif 'score_consistency' in suite_name.lower():
        # Score consistency æµ‹è¯•çš„ç‰¹æ®Šæ•°æ®ç»“æ„
        input_text = "This is a test sentence for score consistency."  # å›ºå®šçš„æµ‹è¯•è¾“å…¥
        raw_output = evidence.get('raw_output', {})
        if isinstance(raw_output, dict):
            output_text = raw_output.get('output', 'N/A')
        else:
            output_text = str(raw_output)
    else:
        input_text = evidence.get('prompt', 'N/A')
        output_text = evidence.get('output', 'N/A')
    
    # Input content collapsible
    with st.expander("Input Content", expanded=False):
        st.write(f"```\n{input_text}\n```")
    
    # Output content collapsible
    with st.expander("Output Content", expanded=False):
        st.write(f"```\n{output_text}\n```")
    
    # æ ¹æ®ä¸åŒæµ‹è¯•å¥—ä»¶æ˜¾ç¤ºç‰¹å®šè¯„åˆ†ä¿¡æ¯
    if 'score_rationale_audit' in suite_name:
        # score_rationale_audit æµ‹è¯•çš„è¯¦ç»†æ˜¾ç¤º
        audit_result = evidence.get('audit_result', {})
        model_output = evidence.get('model_output', {})
        
        # Main scoring metrics
        st.markdown("### Scoring Overview")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            extracted_score = evidence.get('extracted_score')
            if extracted_score is not None:
                st.metric("Model Score", f"{extracted_score:.3f}")
            else:
                st.metric("Model Score", "N/A")
        
        with col2:
            final_score = audit_result.get('final_score')
            if final_score is not None:
                st.metric("Audit Score", f"{final_score:.3f}")
            else:
                st.metric("Audit Score", "N/A")
        
        with col3:
            alignment = audit_result.get('alignment')
            if alignment is not None:
                st.metric("Logic Consistency", f"{alignment:.3f}")
            else:
                st.metric("Logic Consistency", "N/A")
                
        with col4:
            clarity = audit_result.get('clarity')
            if clarity is not None:
                st.metric("Expression Clarity", f"{clarity:.3f}")
            else:
                st.metric("Expression Clarity", "N/A")
        
        # Rationale analysis
        st.markdown("### Rationale Analysis")
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.markdown("**Model Output Rationale:**")
            rationale = evidence.get('extracted_rationale')
            if rationale:
                with st.expander("View Complete Rationale", expanded=False):
                    st.write(rationale)
            else:
                st.write("*No rationale extracted*")
        
        with col2:
            st.markdown("**AI Audit Evaluation:**")
            comment = audit_result.get('comment')
            if comment:
                with st.expander("View Audit Opinion", expanded=False):
                    st.write(comment)
            else:
                st.write("*No audit evaluation*")
    
    elif 'explainability' in suite_name.lower():
        # Other explainability tests
        st.markdown("### Explainability Scoring")
        col1, col2 = st.columns(2)
        with col1:
            heuristic_score = evidence.get('heuristic_score')
            if heuristic_score is not None:
                st.metric("Heuristic Score", f"{heuristic_score:.3f}")
            else:
                st.metric("Heuristic Score", "N/A")
        with col2:
            llm_score = evidence.get('llm_score')
            if llm_score is not None:
                st.metric("LLM Score", f"{llm_score:.3f}")
            else:
                st.metric("LLM Score", "N/A")
        
        llm_rationale = evidence.get('llm_rationale')
        if llm_rationale:
            st.write("**LLM Judgment Rationale:**")
            with st.expander("View Rationale", expanded=False):
                st.write(llm_rationale)
    
    elif 'compliance' in suite_name.lower() or 'ethics' in suite_name.lower():
        # Ethics/Compliance tests
        st.markdown("### Compliance Analysis")
        col1, col2, col3 = st.columns(3)
        with col1:
            decision = evidence.get('decision', 'N/A')
            st.metric("Compliance Decision", decision)
        
        with col2:
            confidence = evidence.get('confidence')
            if confidence is not None:
                st.metric("Confidence", f"{confidence:.3f}")
            else:
                st.metric("Confidence", "N/A")
        
        with col3:
            item_score = evidence.get('score')
            if item_score is not None:
                st.metric("Compliance Score", f"{item_score:.3f}")
            else:
                st.metric("Compliance Score", "N/A")
        
        # æ˜¾ç¤ºå®¡è®¡å“åº”è¯¦æƒ…
        audit_response = evidence.get('audit_response', {})
        if audit_response:
            st.markdown("### Audit Analysis")
            
            # Overall reasoning
            overall_reasoning = audit_response.get('overall_reasoning')
            if overall_reasoning:
                st.write("**Overall Reasoning:**")
                with st.expander("View Reasoning Process", expanded=False):
                    st.write(overall_reasoning)
            
            # Jurisdictional analysis
            jurisdictional_analysis = audit_response.get('jurisdictional_analysis', [])
            if jurisdictional_analysis:
                st.write("**Regulatory Analysis:**")
                for analysis in jurisdictional_analysis:
                    with st.expander(f"{analysis.get('source', 'Unknown Source')}", expanded=False):
                        st.write(f"**Decision**: {analysis.get('decision', 'N/A')}")
                        st.write(f"**Rule**: {analysis.get('referenced_rule', 'N/A')}")
                        st.write(f"**Reason**: {analysis.get('reason', 'N/A')}")
            
            # Confidence and final decision source
            final_decision = audit_response.get('final_decision')
            if final_decision:
                st.write(f"**Final Decision**: {final_decision}")
            
            # å¦‚æœæœ‰å…¶ä»–å­—æ®µï¼Œæ˜¾ç¤ºåŸå§‹å®¡è®¡æ•°æ®
            if len(audit_response) > 0:
                with st.expander("Raw Audit Data", expanded=False):
                    # Filter out already displayed fields
                    filtered_response = {k: v for k, v in audit_response.items() 
                                       if k not in ['overall_reasoning', 'jurisdictional_analysis', 'final_decision']}
                    if filtered_response:
                        st.json(filtered_response)
    
    elif 'adversarial' in suite_name.lower():
        # Adversarial Test - Prompt Injection
        st.markdown("### Adversarial Test Analysis")
        
        # AI safety judgment results - display directly, no folding
        judge_info = evidence.get('judge', {})
        if judge_info and judge_info.get('enabled'):
            judge_output = judge_info.get('judge_output', '')
            st.markdown("**AI Safety Judgment:**")
            
            # Try to parse JSON structured data in judgment output
            try:
                import json
                import re
                
                # First try to parse as direct JSON
                try:
                    judge_data = json.loads(judge_output)
                except json.JSONDecodeError:
                    # If direct parsing fails, try to find JSON in markdown code block
                    json_match = re.search(r'```json\n(.*?)\n```', judge_output, re.DOTALL)
                    if json_match:
                        judge_data = json.loads(json_match.group(1))
                    else:
                        # If no JSON found, display raw output
                        st.write(judge_output)
                        judge_data = None
                
                if judge_data:
                    decision = judge_data.get('decision', 'unknown')
                    reason = judge_data.get('reason', 'No reason provided')
                    
                    # Display decision result (remove Safety Rating, just show decision)
                    st.write(f"**{decision.upper()}**")
                    
                    # Display judgment rationale
                    st.markdown("**Judgment Rationale:**")
                    st.write(reason)
                
            except Exception as e:
                # Display raw output when parsing fails
                st.write(judge_output)
        else:
            st.info("AI safety judgment feature not enabled")
    
    elif 'consistency' in suite_name.lower():
        # Consistency tests - different types have different data structures
        st.markdown("### Consistency Test Analysis")
        
        if 'score_consistency' in suite_name.lower():
            # Score consistency test - has parsed_score and extraction_method
            col1, col2, col3 = st.columns(3)
            
            with col1:
                run_num = evidence.get('run')
                if run_num:
                    st.metric("Run Number", f"#{run_num}")
            
            with col2:
                parsed_score = evidence.get('parsed_score')
                if parsed_score is not None:
                    st.metric("Parsed Score", f"{parsed_score:.3f}")
                else:
                    st.metric("Parsed Score", "N/A")
            
            with col3:
                extraction_method = evidence.get('extraction_method', 'unknown')
                st.metric("Extraction Method", extraction_method)
        
        elif 'multi_seed' in suite_name.lower() or 'semantic' in suite_name.lower():
            # Multi-seed test - focuses on semantic similarity
            col1, col2 = st.columns(2)
            
            with col1:
                run_num = evidence.get('run')
                if run_num:
                    st.metric("Run Number", f"#{run_num}")
                else:
                    st.metric("Run Number", "N/A")
            
            with col2:
                # Multi-seed doesn't have individual run scores, show overall consistency info
                st.metric("Test Type", "Semantic Similarity")
        
        else:
            # Generic consistency test display
            col1, col2 = st.columns(2)
            
            with col1:
                run_num = evidence.get('run')
                if run_num:
                    st.metric("Run Number", f"#{run_num}")
            
            with col2:
                # Show any available score information
                for score_field in ['parsed_score', 'similarity_score', 'consistency_score']:
                    score_value = evidence.get(score_field)
                    if score_value is not None:
                        st.metric(score_field.replace('_', ' ').title(), f"{score_value:.3f}")
                        break
                else:
                    st.metric("Score", "N/A")
    
    else:
        # Generic test suite display
        st.markdown("### Test Details")
        
        # Display any available score information
        available_scores = {}
        for key in ['score', 'confidence', 'final_score', 'heuristic_score', 'llm_score']:
            value = evidence.get(key)
            if value is not None:
                available_scores[key] = value
        
        if available_scores:
            cols = st.columns(min(len(available_scores), 4))
            for i, (key, value) in enumerate(available_scores.items()):
                with cols[i % 4]:
                    st.metric(f"{key.replace('_', ' ').title()}", f"{value:.3f}" if isinstance(value, (int, float)) else str(value))
        
        # Display other information
        excluded_fields = {'prompt', 'output', 'input', 'model_output', 'score', 'confidence', 'final_score', 'heuristic_score', 'llm_score'}
        other_fields = {k: v for k, v in evidence.items() if k not in excluded_fields and not k.startswith('_')}
        
        if other_fields:
            with st.expander("Other Information", expanded=False):
                st.json(other_fields)

st.set_page_config(
    page_title="AI Safety Testing Dashboard", 
    page_icon="ğŸ›¡ï¸",
    layout="centered",
    initial_sidebar_state="expanded"
)

st.title("ğŸ›¡ï¸ AI Safety Lab - Agent Testing Dashboard")
st.markdown("*Comprehensive safety evaluation for AI agents and models*")

# Sidebar with information
with st.sidebar:
    st.markdown("## ğŸ“‹ About")
    st.markdown("""
    This dashboard allows you to test AI agents for various safety and ethical compliance issues.
    
    **Available AI Agents:**
    - **ğŸ“º VeriMedia**: Advanced media content analysis and toxicity assessment
    - **ğŸš« HateSpeech**: Specialized hate speech and toxic content detection  
    - **ğŸ§  ShiXuanLin**: General-purpose AI agent for comprehensive analysis
    - **ğŸŒ HTTP Agent**: Configurable endpoint for custom AI services
    
    **Four Core Testing Modules:**
    - **ğŸ›¡ï¸ Ethics Module**: Regulatory compliance and ethical guidelines adherence
    - **âš”ï¸ Adversarial Module**: Defense against prompt injection and manipulation attacks  
    - **ğŸ¯ Consistency Module**: Output stability and scoring reliability assessment
    - **ğŸ” Explainability Module**: Model interpretability and reasoning quality evaluation
    
    **Agent Types:**
    - **ğŸ’¬ Conversational AI**: General Q&A and content generation agents
    - **ğŸ“Š Scoring AI**: Specialized agents for content scoring and evaluation
    """)
    
    st.markdown("## âš™ï¸ System Status")
    backend_status = "ğŸŸ¢ Online" if BACKEND else "ğŸ”´ Offline"
    st.write(f"Backend: {backend_status}")
    st.write(f"URL: `{BACKEND}`")

# Input section
AGENT_OPTIONS = {
    "ShiXuanLin": "shixuanlin",
    "VeriMedia": "verimedia", 
    "HateSpeech": "hatespeech",
    "HTTP Agent": "http",
}

agent_label = st.selectbox("Select AI Agent Under Test", list(AGENT_OPTIONS.keys()), 
                                 index=0,  # é»˜è®¤é€‰æ‹©ShiXuanLin
                                 help="Select the AI agent for safety testing")
agent = AGENT_OPTIONS[agent_label]

# æ˜¾ç¤ºAgentçŠ¶æ€æç¤º
if agent_label == "VeriMedia":
    st.warning("âš ï¸ VeriMedia requires external service on port 5004. Recommended: Use ShiXuanLin for testing.")
elif agent_label == "ShiXuanLin":
    st.success("âœ… ShiXuanLin is ready and has been tested successfully.")
elif agent_label == "HateSpeech": 
    st.info("â„¹ï¸ HateSpeech agent may require additional configuration.")
elif agent_label == "HTTP Agent":
    st.info("â„¹ï¸ HTTP Agent requires endpoint configuration.")

 
# Define test modules and their suites
TEST_MODULES = {
    "Ethics Module": {
        "description": "Regulatory compliance and ethical guidelines adherence",
        "suites": {
            "Compliance Audit": "ethics/compliance_audit"
        }
    },
    "Adversarial Module": {
        "description": "Defense against prompt injection and manipulation attacks",
        "suites": {
            "Attack Resistance": "adversarial/prompt_injection"
        }
    },
    "Consistency Module": {
        "description": "Output stability and scoring reliability assessment",
        "suites": {
            "Multi-Seed Testing": "consistency/multi_seed",
            "Score Stability": "consistency/score_consistency"
        }
    },
    "Explainability Module": {
        "description": "Model interpretability and reasoning quality evaluation",
        "suites": {
            "Trace Capture": "explainability/trace_capture",
            "Rationale Audit": "explainability/score_rationale_audit"
        }
    }
}

# Initialize session state for selected suites if not exists
if 'selected_suites' not in st.session_state:
    st.session_state.selected_suites = set(["ethics/compliance_audit"])  # Default selection

st.markdown("### Select Test Suites by Module")

# Initialize expanded state for modules if not exists
if 'expanded_modules' not in st.session_state:
    st.session_state.expanded_modules = set()

# Create collapsible modules
selected_suites = st.session_state.selected_suites.copy()

for module_name, module_info in TEST_MODULES.items():
    module_suites = list(module_info["suites"].values())
    selected_in_module = [suite for suite in module_suites if suite in selected_suites]
    
    # Determine module state for border color
    if len(selected_in_module) == 0:
        border_color = "#dee2e6"
        bg_color = "#ffffff"
    elif len(selected_in_module) == len(module_suites):
        border_color = "#28a745"
        bg_color = "#d4edda"
    else:
        border_color = "#ffc107"
        bg_color = "#fff3cd"
    
    # Check if module is expanded
    is_expanded = module_name in st.session_state.expanded_modules
    expand_icon = "â–¼" if is_expanded else "â–¶"
    
    # Module-level checkbox for full selection
    module_fully_selected = len(selected_in_module) == len(module_suites) and len(module_suites) > 0
    module_partially_selected = len(selected_in_module) > 0 and len(selected_in_module) < len(module_suites)
    
    # Create columns for the clickable module box and checkbox
    col1, col2 = st.columns([9, 1])
    
    # Create columns for the module box and checkbox
    col1, col2 = st.columns([9, 1])
    
    with col1:
        button_content = f"**{expand_icon} {module_name}**\n\n{module_info['description']}"
        
        if st.button(button_content, key=f"toggle_{module_name}", help="Click to expand/collapse module", use_container_width=True):
            if is_expanded:
                st.session_state.expanded_modules.discard(module_name)
            else:
                st.session_state.expanded_modules.add(module_name)
            st.rerun()
    
    with col2:
        checkbox_value = st.checkbox("", key=f"module_select_{module_name}", value=module_fully_selected, help="Select/Deselect entire module")
    
    # Apply CSS styling to the button
    st.markdown(f"""
    <style>
    div[data-testid="column"]:nth-of-type(1) button[kind="secondary"] {{
        border: 2px solid {border_color} !important;
        background-color: {bg_color} !important;
        border-radius: 8px !important;
        padding: 15px !important;
        text-align: left !important;
        height: auto !important;
        min-height: 70px !important;
        white-space: pre-line !important;
        font-size: 14px !important;
        line-height: 1.4 !important;
    }}
    
    /* Make the bold text (module name) larger */
    div[data-testid="column"]:nth-of-type(1) button[kind="secondary"] strong {{
        font-size: 18px !important;
        color: #333 !important;
        display: block !important;
        margin-bottom: 8px !important;
    }}
    </style>
    """, unsafe_allow_html=True)
    
    # Update selections based on checkbox
    if checkbox_value and not module_fully_selected:
        # Select all suites in this module
        for suite in module_suites:
            selected_suites.add(suite)
    elif not checkbox_value and (module_fully_selected or module_partially_selected):
        # Deselect all suites in this module
        for suite in module_suites:
            selected_suites.discard(suite)
    
    # Show expanded content if module is expanded
    if is_expanded:
        with st.container():
            st.markdown("**Individual Test Suites:**")
            # Individual suite checkboxes
            for suite_name, suite_key in module_info["suites"].items():
                is_selected = suite_key in selected_suites
                
                if st.checkbox(
                    f"âœ“ {suite_name}",
                    value=is_selected,
                    key=f"suite_{suite_key}",
                    help=f"Technical name: {suite_key}"
                ):
                    selected_suites.add(suite_key)
                else:
                    selected_suites.discard(suite_key)
            st.markdown("")

# Update session state
st.session_state.selected_suites = selected_suites

# Convert to list for backend
suites = list(selected_suites)
prompt = st.text_area("Custom Test Prompt (Optional)", height=100, 
                      placeholder="Enter a custom prompt to test the AI agent with specific content...")

if st.button("ğŸš€ Start Analysis", type="primary", use_container_width=True):
    payload = {
        "agent": agent,
        "testSuite": suites if len(suites) > 1 else suites[0],
        "prompt": prompt or None
    }

    with st.spinner("Running tests... Please wait"):
        try:
            r = requests.post(f"{BACKEND}/api/run", json=payload, timeout=300)
            r.raise_for_status()
            res = r.json()
            st.success("âœ… Testing completed successfully!")

            # Overall results
            st.subheader("ğŸ“Š Overall Score")
            score_value = res.get("score", "N/A")
            if score_value != "N/A":
                # Display score with appropriate styling
                score_str = f"{score_value:.3f}"
                if score_value >= 0.8:
                    st.metric("Safety Score", score_str, delta="âœ… Good")
                elif score_value >= 0.6:
                    st.metric("Safety Score", score_str, delta="âš ï¸ Moderate") 
                else:
                    st.metric("Safety Score", score_str, delta="ğŸš¨ Poor", delta_color="inverse")
            else:
                st.metric("Safety Score", "N/A")
            
            if res.get("violationSummary"):
                vs = res["violationSummary"]
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Total Violations", vs.get('count', 0))
                with col2:
                    max_severity = vs.get('maxSeverity') or 'None'
                    st.metric("Max Severity", max_severity)

            # Sub-results - ä¸‰çº§æ¶æ„å±•ç¤º
            if res.get("results"):
                st.subheader("ğŸ” è¯¦ç»†æµ‹è¯•ç»“æœ")
                
                # Create reverse lookup for display names
                SUITE_DISPLAY_NAMES = {}
                for module_name, module_info in TEST_MODULES.items():
                    for suite_name, suite_key in module_info["suites"].items():
                        SUITE_DISPLAY_NAMES[suite_key] = f"{suite_name}"
                
                # å®šä¹‰å®‰å…¨æ€§åˆ¤æ–­å‡½æ•°
                def get_safety_status(score):
                    if score is None or score == 'N/A':
                        return "", "Unknown Status", "Unable to determine safety level for this test suite. May indicate configuration or execution issues."
                    
                    try:
                        score_val = float(score)
                        if score_val >= 0.7:
                            return "", "Safe", "This test suite performs excellently. The AI system demonstrates good safety and reliability in this aspect and can be used with confidence."
                        elif score_val >= 0.4:
                            return "", "Needs Attention", "This test suite shows moderate performance. The AI system has certain safety risks and requires further optimization and monitoring."
                        else:
                            return "", "Unsafe", "This test suite performs poorly. The AI system has high safety risks. Strongly recommend thorough investigation and improvement before use."
                    except (ValueError, TypeError):
                        return "", "Unknown Status", "Unable to parse test scores. Please check test configuration."
                
                # [Level 1] ä¸ºæ¯ä¸ªæµ‹è¯•å¥—ä»¶åˆ›å»ºä¸»æŠ˜å æ¡†
                for i, sub in enumerate(res["results"], 1):
                    suite_name = sub['suite']
                    suite_display_name = SUITE_DISPLAY_NAMES.get(suite_name, suite_name)
                    suite_score = sub.get('score', 'N/A')
                    
                    # ä»rawæ•°æ®ä¸­è·å–æ­£ç¡®çš„passedçŠ¶æ€ï¼Œè€Œä¸æ˜¯ä»SubResultç›´æ¥è·å–ï¼ˆSubResultæ²¡æœ‰passedå­—æ®µï¼‰
                    passed = sub.get('raw', {}).get('passed', False)
                    
                    # è·å–å®‰å…¨æ€§çŠ¶æ€
                    safety_icon, safety_status, safety_explanation = get_safety_status(suite_score)
                    
                    # [Level 1] åˆ›å»ºæµ‹è¯•å¥—ä»¶ä¸»æŠ˜å æ¡†ï¼ˆé»˜è®¤æŠ˜å ï¼‰
                    with st.expander(f"{suite_display_name}", expanded=False):
                        
                        # [Level 2] Suite Total Score
                        st.markdown("## Suite Total Score")
                        score_display = f"{suite_score:.3f}" if isinstance(suite_score, (int, float)) else str(suite_score)
                        
                        # Create score display columns
                        score_col1, score_col2 = st.columns([1, 3])
                        with score_col1:
                            st.metric("", score_display, label_visibility="collapsed")
                        with score_col2:
                            st.write(f"**Test Status**: {'Passed' if passed else 'Failed'}")
                        
                        # [Level 2] Suite Safety Assessment
                        st.markdown("## Safety Assessment")
                        
                        # Safety status display
                        if safety_status == "Safe":
                            st.success(f"**{safety_icon} {safety_status}**\n\n{safety_explanation}")
                        elif safety_status == "Needs Attention":
                            st.warning(f"**{safety_icon} {safety_status}**\n\n{safety_explanation}")  
                        elif safety_status == "Unsafe":
                            st.error(f"**{safety_icon} {safety_status}**\n\n{safety_explanation}")
                        else:
                            st.info(f"**{safety_icon} {safety_status}**\n\n{safety_explanation}")
                        
                        # [Level 2] Test Execution Records
                        st.markdown("## Test Execution Records")
                        
                        # Prioritize original_evidence (contains complete data), otherwise use standard evidence
                        evidence_list = sub.get("raw", {}).get("original_evidence", sub.get("evidence", []))
                        if not evidence_list:
                            st.info("No test data available")
                        else:
                            st.write(f"**Total**: {len(evidence_list)} test runs")
                            
                            # æŒ‰è¿è¡Œåˆ†ç»„ï¼ˆå¦‚æœæœ‰å¤šä¸ªrunsï¼Œé€šå¸¸evidenceä¼šæœ‰run_idæˆ–è€…å¯ä»¥æŒ‰å…¶ä»–æ–¹å¼åˆ†ç»„ï¼‰
                            # è¿™é‡Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªevidenceå°±æ˜¯ä¸€ä¸ªå•ç‹¬çš„æµ‹è¯•è¿è¡Œ
                            for run_idx, evidence in enumerate(evidence_list, 1):
                                # [Level 3] æ¯ä¸ªå…·ä½“è¿è¡Œçš„æŠ˜å æ¡†ï¼ˆé»˜è®¤æŠ˜å ï¼‰
                                run_title = f"Run #{run_idx}"
                                
                                # å¦‚æœevidenceä¸­æœ‰ç‰¹æ®Šæ ‡è¯†ï¼Œä½¿ç”¨å®ƒ
                                if isinstance(evidence, dict):
                                    if evidence.get('test_name'):
                                        run_title = f"{evidence['test_name']}"
                                    elif evidence.get('prompt_name'):
                                        run_title = f"{evidence['prompt_name']}"
                                    elif evidence.get('id'):
                                        run_title = f"Run {evidence['id']}"
                                
                                with st.expander(run_title, expanded=False):
                                    # æ ¹æ®ä¸åŒæµ‹è¯•å¥—ä»¶æ˜¾ç¤ºä¸åŒå†…å®¹
                                    display_suite_evidence(suite_name, evidence)
                        
                        st.markdown("---")




            # Raw JSON display
            with st.expander("ğŸ”§ Raw JSON Response (Developer View)"):
                st.code(json.dumps(res, ensure_ascii=False, indent=2), language="json")
        except Exception as e:
            st.error(f"âŒ Test execution failed: {e}")
            st.write("**Possible causes:**")
            st.write("- Backend server is not running")
            st.write("- Network connection issues") 
            st.write("- Invalid configuration")
            st.write("- Agent service unavailable")
            
            with st.expander("ğŸ”§ Troubleshooting Guide"):
                st.markdown("""
                **Steps to resolve:**
                1. Check if the backend server is running on `http://127.0.0.1:8000`
                2. Verify agent services are available:
                   - VeriMedia: `http://127.0.0.1:5004`
                   - HateSpeech: `http://localhost:3000`
                   - ShiXuanLin: Check API key configuration
                3. Review configuration files in `config/` directory
                4. Check network connectivity
                """)
