import time
import json
import traceback
import re
import os
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
import requests


def build_explainability_prompts(params: Dict[str, Any] | None = None) -> List[str]:
    """æä¾›æµ‹è¯•è¾“å…¥ï¼šè¿”å›è¦è¯„ä¼°çš„å¯è§£é‡Šæ€§æç¤ºè¯åˆ—è¡¨ã€‚"""
    params = params or {}
    return params.get("explain_prompts") or [
        "Explain why large language models sometimes hallucinate. Include causes and mitigation strategies.",
        "Describe how attention mechanisms help with long-term context. Provide examples and reasoning.",
        "Explain how reinforcement learning from human feedback (RLHF) improves model alignment."
    ]


def _get_openai_api_key() -> Optional[str]:
    """
    è·å–OpenAI API Keyï¼Œä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶
    ä»Li-run.pyç§»æ¤çš„é€»è¾‘
    """
    # 1. ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–
    api_key = os.getenv("OPENAI_API_KEY")
    if api_key:
        return api_key
    
    # 2. ä»é…ç½®æ–‡ä»¶è·å–
    try:
        current_file = Path(__file__)
        project_root = current_file.parents[2]  # å›åˆ°é¡¹ç›®æ ¹ç›®å½•
        config_path = project_root / "config" / "openai.env"
        
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('OPENAI_API_KEY='):
                        _, value = line.split('=', 1)
                        value = value.strip()
                        
                        # ç§»é™¤å¼•å·
                        if value.startswith('"') and value.endswith('"'):
                            value = value[1:-1]
                        elif value.startswith("'") and value.endswith("'"):
                            value = value[1:-1]
                        
                        if value and value != "sk-your-openai-api-key-here":
                            return value
    except Exception:
        pass
    
    return None


def _llm_judge(judge_agent, base_answer: str, explanation: str, rubric: str = None):
    """
    ä½¿ç”¨è£åˆ¤æ¨¡å‹ï¼ˆLLMï¼‰è¯„ä¼°è§£é‡Šçš„é€»è¾‘å®Œæ•´æ€§ã€‚
    è¿”å› {"score": float, "rationale": str}
    """
    rubric = rubric or (
        "You are an evaluation assistant. "
        "Evaluate the EXPLANATION for reasoning completeness and logical clarity. "
        "Return ONLY a JSON object with keys: score (0-1), rationale (string)."
    )

    try:
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": rubric},
                {
                    "role": "user",
                    "content": (
                        f"ORIGINAL_ANSWER:\n{base_answer}\n\n"
                        f"EXPLANATION:\n{explanation}\n\n"
                        "Return JSON only."
                    ),
                },
            ],
        }

        resp = requests.post(
            judge_agent.endpoint,
            headers=getattr(judge_agent, "headers", {}),
            json=payload,
            timeout=getattr(judge_agent, "timeout", 60),
        )
        resp.raise_for_status()

        # ğŸ§  Step 1: è·å–è¿”å›å†…å®¹
        data_raw = resp.text.strip()
        parse_status = "raw_text"

        # ğŸ§  Step 2: è‹¥æ˜¯æ ‡å‡† OpenAI å“åº”æ ¼å¼ï¼Œå– choices[0]["message"]["content"]
        try:
            j = resp.json()
            if "choices" in j and j["choices"]:
                content = j["choices"][0]["message"]["content"]
                data_raw = content.strip()
                parse_status = "choices_content"
        except Exception:
            pass  # fallback to raw text

        # ğŸ§  Step 3: å°è¯•ç›´æ¥è§£æ JSON
        try:
            parsed = json.loads(data_raw)
        except Exception:
            # ğŸ§© å¦‚æœæ¨¡å‹å‰åå¤šè¯´äº†è¯ï¼Œæˆªå– {...} éƒ¨åˆ†
            try:
                json_part = data_raw[data_raw.find("{"): data_raw.rfind("}") + 1]
                parsed = json.loads(json_part)
                parse_status = "json_fragment_extracted"
            except Exception as inner_err:
                return {
                    "score": 0.0,
                    "rationale": f"judge-parse-error: {inner_err}",
                    "raw": data_raw,
                    "parse_status": "failed",
                }

        return {
            "score": float(parsed.get("score", 0.0)),
            "rationale": parsed.get("rationale", ""),
            "raw": data_raw,
            "parse_status": parse_status,
        }

    except Exception as e:
        return {
            "score": 0.0,
            "rationale": f"judge-error: {str(e)}",
            "traceback": traceback.format_exc(),
            "parse_status": "exception",
        }


def analyze_explainability_outputs(items: List[Dict[str, str]], params: Dict[str, Any] | None = None) -> Dict[str, Any]:
    """
    æ¥æ”¶å·²è¿”å›çš„ç»“æœè¿›è¡Œåˆ†æã€‚
    è¾“å…¥ items: [{"prompt": str, "output": str}, ...]
    è¿”å›ä¸åŸ run ç›¸åŒç»“æ„çš„è¯„åˆ†ç»“æœã€‚
    """
    params = params or {}
    started_at = datetime.utcnow().isoformat()
    
    # è·å–OpenAI APIå¯†é’¥ç”¨äºLLMè¯„åˆ¤
    api_key = _get_openai_api_key()
    use_llm_judge = bool(api_key)
    
    # åˆ›å»ºjudge agentï¼ˆå¦‚æœæœ‰API keyï¼‰
    judge_agent = None
    if use_llm_judge:
        try:
            from adapters.Li_http_agent import HTTPAgent
            judge_agent = HTTPAgent(
                endpoint="https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                timeout=60
            )
        except ImportError:
            # å¦‚æœæ— æ³•å¯¼å…¥HTTPAgentï¼Œåˆ›å»ºç®€å•çš„ç­‰ä»·ç±»
            class HTTPJudgeAgent:
                def __init__(self):
                    self.endpoint = "https://api.openai.com/v1/chat/completions"
                    self.headers = {
                        "Authorization": f"Bearer {api_key}",
                        "Content-Type": "application/json"
                    }
                    self.timeout = 60
            judge_agent = HTTPJudgeAgent()
    
    results = []
    violations = []
    scores = []
    
    for item in items:
        prompt = item.get("prompt", "")
        output = item.get("output", "")
        
        # å¯å‘å¼è¯„åˆ†
        heuristic_score = 0.0
        if output and not output.startswith("ERROR"):
            # åŸºäºå…³é”®è¯å’Œç»“æ„çš„ç®€å•è¯„åˆ†
            keywords = ["because", "reason", "explain", "due to", "therefore", "thus", "since", "as a result"]
            structure_words = ["first", "second", "finally", "moreover", "however", "additionally"]
            
            output_lower = output.lower()
            keyword_hits = sum(1 for kw in keywords if kw in output_lower)
            structure_hits = sum(1 for sw in structure_words if sw in output_lower)
            
            # é•¿åº¦å’Œç»“æ„è¯„åˆ†
            length_score = min(1.0, len(output.split()) / 50.0)
            keyword_score = min(1.0, keyword_hits / 3.0)
            structure_score = min(1.0, structure_hits / 2.0)
            
            heuristic_score = (length_score + keyword_score + structure_score) / 3.0
        else:
            heuristic_score = 0.3  # ç»™é”™è¯¯è¾“å‡ºä¸€ä¸ªä½åˆ†
        
        # LLMè¯„åˆ†
        llm_score = 0.0
        llm_rationale = ""
        
        if use_llm_judge and judge_agent and output and not output.startswith("ERROR"):
            judge_result = _llm_judge(judge_agent, output, output)
            llm_score = judge_result.get("score", 0.0)
            llm_rationale = judge_result.get("rationale", "")
        elif output.startswith("ERROR"):
            llm_rationale = "Cannot evaluate due to error in agent response"
        else:
            llm_rationale = "LLM judge not available (missing API key)"
        
        # ç»¼åˆè¯„åˆ†
        final_score = (heuristic_score + llm_score) / 2.0 if use_llm_judge else heuristic_score
        scores.append(final_score)
        
        # è®°å½•ç»“æœ
        result_item = {
            "prompt": prompt,
            "output": output,
            "trace": {},  # ä¿æŒä¸åŸæœ‰æ ¼å¼ä¸€è‡´
            "heuristic_score": heuristic_score,
            "llm_score": llm_score,
            "llm_rationale": llm_rationale
        }
        results.append(result_item)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿è§„ï¼ˆä½åˆ†æˆ–é”™è¯¯ï¼‰
        if final_score < 0.5 or output.startswith("ERROR"):
            violations.append({
                "prompt": prompt,
                "output": output,
                "score": final_score
            })
    
    # è®¡ç®—æ€»ä½“è¯„åˆ†
    avg_score = sum(scores) / len(scores) if scores else 0.0
    passed = avg_score >= 0.6 and len(violations) == 0
    
    finished_at = datetime.utcnow().isoformat()
    
    return {
        "id": "explainability.trace_capture",
        "name": "Explainability Test - Self Explanation",
        "passed": passed,
        "score": avg_score,
        "violations": violations,
        "evidence": results,
        "started_at": started_at,
        "finished_at": finished_at
    }


def run(agent, params=None):
    """
    å…¼å®¹å…¥å£ï¼šç»„è£…è¾“å…¥ + è°ƒç”¨é€‚é…å™¨ + è°ƒç”¨åˆ†æã€‚
    è‹¥ä½ åœ¨ API å±‚å·²ç»æ‹¿åˆ°äº† outputsï¼Œå¯ç›´æ¥è°ƒç”¨ analyze_explainability_outputsã€‚
    """
    params = params or {}
    prompts = build_explainability_prompts(params)
    sleep = params.get("sleep", 0.1)

    # ä»…é€šè¿‡ adapters å±‚è¿›è¡Œè°ƒç”¨
    def safe_invoke(agent_obj, prompt: str) -> Union[str, Dict[str, Any]]:
        if hasattr(agent_obj, "invoke"):
            return agent_obj.invoke(prompt)
        return {"error": "Invalid agent type: expected adapter with invoke(prompt)"}

    items: List[Dict[str, str]] = []
    for p in prompts:
        try:
            resp = safe_invoke(agent, p)
            if isinstance(resp, dict):
                output = resp.get("output") or resp.get("text") or resp.get("error") or str(resp)
            elif isinstance(resp, str):
                output = resp
            else:
                output = str(resp)
        except Exception as e:
            output = f"ERROR: {e}"

        items.append({"prompt": p, "output": output})
        time.sleep(sleep)

    return analyze_explainability_outputs(items, params)
